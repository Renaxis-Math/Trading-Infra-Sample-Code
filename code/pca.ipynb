{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import consts\n",
    "import helper\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn, scipy, requests\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = consts.RAW_DATA_PATH_RYAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(helper)\n",
    "# df = pd.read_csv(FILE_PATH + 'data.20211223_1200')\n",
    "# df = df[df.columns[8:]] # cut out the test data\n",
    "# helper.pca_plot(df, num_components=244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(FILE_PATH + 'data.20200316_1200')\n",
    "# df.columns\n",
    "# df = df[df.columns[7:]] # cut out testing columns\n",
    "# helper.pca_plot(df, num_components=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = \"RYAN\"\n",
    "RESPONSE_NAME = consts.RESPONSE_NAME\n",
    "\n",
    "LASSO = \"LASSO\"\n",
    "XGBOOST = \"XGBOOST\"\n",
    "\n",
    "ALPHA = 0.05 # Set significance level\n",
    "\n",
    "TEST_START  = \"20191001\"\n",
    "TRAIN_TEST_GAP = 31 # days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory ../../clinicDataCopy/ does not exist.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m importlib\u001b[38;5;241m.\u001b[39mreload(helper)\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mhelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFILE_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_df \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mupdate_and_get_train_df(TEST_START, backward_dayCount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m31\u001b[39m, train_data_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m)\n\u001b[1;32m      5\u001b[0m response_cols \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39mcolumns[:\u001b[38;5;241m7\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Clinic/code_final/helper.py:519\u001b[0m, in \u001b[0;36mData.__init__\u001b[0;34m(self, train_data_path, train_data, test_data)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path \u001b[38;5;241m=\u001b[39m train_data_path\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msorted_file_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_sorted_file_names(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_path)\n\u001b[0;32m--> 519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msorted_file_datetimes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_sorted_file_datetimes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_df \u001b[38;5;241m=\u001b[39m train_data\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dfs \u001b[38;5;241m=\u001b[39m test_data\n",
      "File \u001b[0;32m~/Documents/Clinic/code_final/helper.py:726\u001b[0m, in \u001b[0;36mData._init_sorted_file_datetimes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_sorted_file_datetimes\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[datetime]:\n\u001b[1;32m    724\u001b[0m     answers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 726\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msorted_file_names:\n\u001b[1;32m    727\u001b[0m         file_datetime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_datetime(file_name)\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_datetime \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: answers\u001b[38;5;241m.\u001b[39mappend(file_datetime)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "data = helper.Data(train_data_path=FILE_PATH)\n",
    "\n",
    "train_df = data.update_and_get_train_df(TEST_START, backward_dayCount = 31, train_data_count=182)\n",
    "response_cols = train_df.columns[:7]\n",
    "\n",
    "# 6 months\n",
    "\n",
    "# cut out response columns\n",
    "validation_data = data.train_df[RESPONSE_NAME]\n",
    "\n",
    "#get the transformed training data\n",
    "\n",
    "# removing high variance features\n",
    "removable_features = []\n",
    "\n",
    "highCorr_features_map = data.find_high_corr(.6)\n",
    "\n",
    "for _, highCorr_pairs in highCorr_features_map.items():\n",
    "    for (feature1, feature2) in highCorr_pairs:\n",
    "        insig_features = helper.hypothesis_test_features(data.train_df, feature1, feature2, alpha = .01)\n",
    "        removable_features.extend(insig_features)\n",
    "\n",
    "data.train_df.drop(removable_features, axis = consts.COL, inplace=True)\n",
    "\n",
    "# cut out response columns\n",
    "for col in data.train_df.columns:\n",
    "    if col in response_cols:\n",
    "        data.train_df.drop([col], axis = consts.COL, inplace=True)\n",
    "\n",
    "train_df = data.train_df\n",
    "print(train_df.shape)\n",
    "test_dfs = data.update_and_get_test_df(data_path = FILE_PATH, start_date=TEST_START, end_date=\"20200301\")\n",
    "important_cols = train_df.columns\n",
    "test_xs = [df[important_cols] for df in test_dfs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "\n",
    "# # Scale the data\n",
    "scalar = StandardScaler()\n",
    "train_df = scalar.fit_transform(train_df) \n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=37)\n",
    "pca.fit(train_df)\n",
    "\n",
    "# Fit Transform X training data\n",
    "train_df_pca = pd.DataFrame(pca.transform(train_df))\n",
    "# add on the original y values \n",
    "train_df_pca[RESPONSE_NAME] = validation_data.values # leave untransformed\n",
    "\n",
    "scaled_test = [pd.DataFrame(scalar.fit_transform(df)) for df in test_xs]\n",
    "transformed_test = [pd.DataFrame(pca.transform(df)) for df in scaled_test]\n",
    "test_dfs_pca = []\n",
    "for pca_df, df in zip(transformed_test, test_dfs):\n",
    "    pca_df[RESPONSE_NAME] = df[RESPONSE_NAME]\n",
    "    test_dfs_pca.append(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate cumulative explained variance ratio\n",
    "# cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# # Plot the cumulative explained variance ratio\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(np.arange(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='-')\n",
    "# plt.title('Cumulative Explained Variance Ratio')\n",
    "# plt.xlabel('Number of Principal Components')\n",
    "# plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using: OLS.\n",
      "Remember: Model Class works with 1 training data and N testing data.\n",
      "Your model's DEFAULT init hyperparams are: {'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}\n",
      "No. features being used: 37\n",
      "response_corr: 0.0693776759369518\n",
      "mean_return: 0.0005688567546702572\n",
      "scale_factor: 1.3431570184806756\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "full_ols_model = helper.Model('OLS')\n",
    "full_ols_model.train(pd.DataFrame(train_df_pca))\n",
    "predictions = full_ols_model.test(test_dfs_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using: LASSO.\n",
      "Remember: Model Class works with 1 training data and N testing data.\n",
      "Your model's DEFAULT init hyperparams are: {'alphas': None, 'copy_X': True, 'cv': None, 'eps': 0.001, 'fit_intercept': True, 'max_iter': 1000, 'n_alphas': 100, 'n_jobs': None, 'positive': False, 'precompute': 'auto', 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'verbose': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. features being used: 37\n",
      "response_corr: 0.07051148160375313\n",
      "mean_return: 0.0005668209979404081\n",
      "scale_factor: 1.6616803360885477\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "lasso_model = helper.Model(LASSO)\n",
    "lasso_model.train(pd.DataFrame(train_df_pca))\n",
    "predictions = lasso_model.test(test_dfs_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using: XGBOOST.\n",
      "Remember: Model Class works with 1 training data and N testing data.\n",
      "Your model's DEFAULT init hyperparams are: {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "No. features being used: 37\n",
      "response_corr: 0.013083858719121644\n",
      "mean_return: 0.0002282102724318523\n",
      "scale_factor: 0.07882485538721085\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "xgboost_model = helper.Model(XGBOOST)\n",
    "xgboost_model.train(pd.DataFrame(train_df_pca))\n",
    "predictions = xgboost_model.test(test_dfs_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
