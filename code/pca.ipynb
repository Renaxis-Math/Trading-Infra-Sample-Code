{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import consts\n",
    "import helper\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn, scipy, requests\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = consts.RAW_DATA_PATH_RYAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(helper)\n",
    "# df = pd.read_csv(FILE_PATH + 'data.20211223_1200')\n",
    "# df = df[df.columns[8:]] # cut out the test data\n",
    "# helper.pca_plot(df, num_components=244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(FILE_PATH + 'data.20200316_1200')\n",
    "# df.columns\n",
    "# df = df[df.columns[7:]] # cut out testing columns\n",
    "# helper.pca_plot(df, num_components=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER = \"RYAN\"\n",
    "RESPONSE_NAME = consts.RESPONSE_NAME\n",
    "\n",
    "LASSO = \"LASSO\"\n",
    "XGBOOST = \"XGBOOST\"\n",
    "\n",
    "ALPHA = 0.05 # Set significance level\n",
    "\n",
    "TEST_START  = \"20191001\"\n",
    "TRAIN_TEST_GAP = 31 # days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No YYYYMMDD datetime matched.\n",
      "\n",
      "Getting files from 2019-03-02 00:00:00 to 2019-08-31 00:00:00, inclusive.\n",
      "Filtered File Dates: ['data.20190304_1200', 'data.20190305_1200', 'data.20190306_1200', 'data.20190307_1200', 'data.20190308_1200', 'data.20190311_1200', 'data.20190312_1200', 'data.20190313_1200', 'data.20190314_1200', 'data.20190315_1200', 'data.20190318_1200', 'data.20190319_1200', 'data.20190320_1200', 'data.20190321_1200', 'data.20190322_1200', 'data.20190325_1200', 'data.20190326_1200', 'data.20190327_1200', 'data.20190328_1200', 'data.20190329_1200', 'data.20190401_1200', 'data.20190402_1200', 'data.20190403_1200', 'data.20190404_1200', 'data.20190405_1200', 'data.20190408_1200', 'data.20190409_1200', 'data.20190410_1200', 'data.20190411_1200', 'data.20190412_1200', 'data.20190415_1200', 'data.20190416_1200', 'data.20190417_1200', 'data.20190418_1200', 'data.20190422_1200', 'data.20190423_1200', 'data.20190424_1200', 'data.20190425_1200', 'data.20190426_1200', 'data.20190429_1200', 'data.20190430_1200', 'data.20190501_1200', 'data.20190502_1200', 'data.20190503_1200', 'data.20190506_1200', 'data.20190507_1200', 'data.20190508_1200', 'data.20190509_1200', 'data.20190510_1200', 'data.20190513_1200', 'data.20190514_1200', 'data.20190515_1200', 'data.20190516_1200', 'data.20190517_1200', 'data.20190520_1200', 'data.20190521_1200', 'data.20190522_1200', 'data.20190523_1200', 'data.20190524_1200', 'data.20190528_1200', 'data.20190529_1200', 'data.20190530_1200', 'data.20190531_1200', 'data.20190603_1200', 'data.20190604_1200', 'data.20190605_1200', 'data.20190606_1200', 'data.20190607_1200', 'data.20190610_1200', 'data.20190611_1200', 'data.20190612_1200', 'data.20190613_1200', 'data.20190614_1200', 'data.20190617_1200', 'data.20190618_1200']\n",
      "\n",
      "(220393, 167)\n",
      "Getting files from 2019-10-01 00:00:00 to 2020-03-01 00:00:00, inclusive.\n",
      "Filtered File Dates: ['data.20200102_1200', 'data.20200103_1200', 'data.20200106_1200', 'data.20200107_1200', 'data.20200108_1200', 'data.20200109_1200', 'data.20200110_1200', 'data.20200113_1200', 'data.20200114_1200', 'data.20200115_1200', 'data.20200116_1200', 'data.20200117_1200', 'data.20200121_1200', 'data.20200122_1200', 'data.20200123_1200', 'data.20200124_1200', 'data.20200127_1200', 'data.20200128_1200', 'data.20200129_1200', 'data.20200130_1200', 'data.20200131_1200', 'data.20200203_1200', 'data.20200204_1200', 'data.20200205_1200', 'data.20200206_1200', 'data.20200207_1200', 'data.20200210_1200', 'data.20200211_1200', 'data.20200212_1200', 'data.20200213_1200', 'data.20200214_1200', 'data.20200218_1200', 'data.20200219_1200', 'data.20200220_1200', 'data.20200221_1200', 'data.20200224_1200', 'data.20200225_1200', 'data.20200226_1200', 'data.20200227_1200', 'data.20200228_1200']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "data = helper.Data(train_data_path=FILE_PATH)\n",
    "\n",
    "train_df = data.update_and_get_train_df(TEST_START, backward_dayCount = 31, train_data_count=182)\n",
    "response_cols = train_df.columns[:7]\n",
    "\n",
    "# 6 months\n",
    "\n",
    "# cut out response columns\n",
    "validation_data = data.train_df[RESPONSE_NAME]\n",
    "\n",
    "#get the transformed training data\n",
    "\n",
    "# removing high variance features\n",
    "removable_features = []\n",
    "\n",
    "highCorr_features_map = data.find_high_corr(.6)\n",
    "\n",
    "for _, highCorr_pairs in highCorr_features_map.items():\n",
    "    for (feature1, feature2) in highCorr_pairs:\n",
    "        insig_features = helper.hypothesis_test_features(data.train_df, feature1, feature2, alpha = .01)\n",
    "        removable_features.extend(insig_features)\n",
    "\n",
    "data.train_df.drop(removable_features, axis = consts.COL, inplace=True)\n",
    "\n",
    "# cut out response columns\n",
    "for col in data.train_df.columns:\n",
    "    if col in response_cols:\n",
    "        data.train_df.drop([col], axis = consts.COL, inplace=True)\n",
    "\n",
    "train_df = data.train_df\n",
    "print(train_df.shape)\n",
    "test_dfs = data.update_and_get_test_df(data_path = FILE_PATH, start_date=TEST_START, end_date=\"20200301\")\n",
    "important_cols = train_df.columns\n",
    "test_xs = [df[important_cols] for df in test_dfs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "\n",
    "# # Scale the data\n",
    "scalar = StandardScaler()\n",
    "train_df = scalar.fit_transform(train_df) \n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=37)\n",
    "pca.fit(train_df)\n",
    "\n",
    "# Fit Transform X training data\n",
    "train_df_pca = pd.DataFrame(pca.transform(train_df))\n",
    "# add on the original y values \n",
    "train_df_pca[RESPONSE_NAME] = validation_data.values # leave untransformed\n",
    "\n",
    "scaled_test = [pd.DataFrame(scalar.fit_transform(df)) for df in test_xs]\n",
    "transformed_test = [pd.DataFrame(pca.transform(df)) for df in scaled_test]\n",
    "test_dfs_pca = []\n",
    "for pca_df, df in zip(transformed_test, test_dfs):\n",
    "    pca_df[RESPONSE_NAME] = df[RESPONSE_NAME]\n",
    "    test_dfs_pca.append(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate cumulative explained variance ratio\n",
    "# cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# # Plot the cumulative explained variance ratio\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(np.arange(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='-')\n",
    "# plt.title('Cumulative Explained Variance Ratio')\n",
    "# plt.xlabel('Number of Principal Components')\n",
    "# plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using: OLS.\n",
      "Remember: Model Class works with 1 training data and N testing data.\n",
      "Your model's DEFAULT init hyperparams are: {'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}\n",
      "No. features being used: 37\n",
      "response_corr: 0.0693776759369518\n",
      "mean_return: 0.0005688567546702572\n",
      "scale_factor: 1.3431570184806756\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "full_ols_model = helper.Model('OLS')\n",
    "full_ols_model.train(pd.DataFrame(train_df_pca))\n",
    "predictions = full_ols_model.test(test_dfs_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using: LASSO.\n",
      "Remember: Model Class works with 1 training data and N testing data.\n",
      "Your model's DEFAULT init hyperparams are: {'alphas': None, 'copy_X': True, 'cv': None, 'eps': 0.001, 'fit_intercept': True, 'max_iter': 1000, 'n_alphas': 100, 'n_jobs': None, 'positive': False, 'precompute': 'auto', 'random_state': None, 'selection': 'cyclic', 'tol': 0.0001, 'verbose': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. features being used: 37\n",
      "response_corr: 0.07051148160375313\n",
      "mean_return: 0.0005668209979404081\n",
      "scale_factor: 1.6616803360885477\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "lasso_model = helper.Model(LASSO)\n",
    "lasso_model.train(pd.DataFrame(train_df_pca))\n",
    "predictions = lasso_model.test(test_dfs_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're using: XGBOOST.\n",
      "Remember: Model Class works with 1 training data and N testing data.\n",
      "Your model's DEFAULT init hyperparams are: {'objective': 'reg:squarederror', 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': None, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "No. features being used: 37\n",
      "response_corr: 0.013083858719121644\n",
      "mean_return: 0.0002282102724318523\n",
      "scale_factor: 0.07882485538721085\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(helper)\n",
    "xgboost_model = helper.Model(XGBOOST)\n",
    "xgboost_model.train(pd.DataFrame(train_df_pca))\n",
    "predictions = xgboost_model.test(test_dfs_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
