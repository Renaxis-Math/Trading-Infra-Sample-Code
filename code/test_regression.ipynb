{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper, consts\n",
    "import importlib\n",
    "importlib.reload(consts)\n",
    "importlib.reload(helper)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn, scipy, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: OLS\n",
      "2: LASSO\n",
      "3: XGBOOST\n"
     ]
    }
   ],
   "source": [
    "# USER\n",
    "USER = \"HOANG\"\n",
    "FILE_PATH = consts.PATH_MAP[USER]\n",
    "\n",
    "ROW = consts.ROW\n",
    "COL = consts.COL\n",
    "CLEANED_DATA_PATH = consts.CLEANED_DATA_PATH\n",
    "DATA_PATH = consts.RAW_DATA_PATH\n",
    "RESPONSE_NAME = consts.RESPONSE_NAME\n",
    "\n",
    "TRAIN_START_DATE = \"20150101\"\n",
    "TRAIN_END_DATE = \"20150601\" # Up to but not including\n",
    "TEST_START_DATE = \"20150701\"\n",
    "TEST_END_DATE = \"20150801\"\n",
    "\n",
    "REGRESSION_TYPES = helper.Regression('OLS').list_all_regression_types()\n",
    "REGRESSION_TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start, train_end  = helper.get_train_from_testday(TEST_START_DATE)\n",
    "list_of_interacting_terms = [[\"relvol_nt_0\",\"rrirpnxm_nt_0\"], \n",
    "                            [\"relvol_lst15_0\", \"rrirpnxm_lst15_0\"],\n",
    "                            [\"relvol_lsthrx15_0\", \"rrirpnxm_lsthrx15_0\"],\n",
    "                            [\"relvol_toxhr_0\", \"rrirpnxm_toxhr_0\"]]\n",
    "\n",
    "FEATURE_COL_NAMES = [\"rrirpnxm_nt_0\", \"rrirpnxm_lst15_0\", \"rrirpnxm_lsthrx15_0\", \"rrirpnxm_toxhr_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = helper.get_file_names(TEST_START_DATE, TEST_END_DATE, FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file_name in file_names:\n",
    "    df = pd.read_csv(FILE_PATH + file_name)\n",
    "    df, new_col_names = helper.get_df_with_interaction_terms(df, list_of_interacting_terms)\n",
    "    stayed_cols = np.append(FEATURE_COL_NAMES, new_col_names)\n",
    "    stayed_cols = np.append(stayed_cols, consts.RESPONSE_NAME)\n",
    "    dfs.append(df[stayed_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y_list = []\n",
    "actual_y_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for df in dfs:\n",
    "    has_intercept_df = helper.append_columnOf_ones(df)\n",
    "\n",
    "    y = has_intercept_df[consts.RESPONSE_NAME]\n",
    "    X = has_intercept_df.drop(consts.RESPONSE_NAME, inplace=False, axis=consts.COL)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, train_size = .8, random_state = 42)\n",
    "    actual_y_list.append(test_y)\n",
    "    \n",
    "    from sklearn import linear_model\n",
    "    from xgboost import XGBRegressor\n",
    "    # model = linear_model.LinearRegression(fit_intercept=False)\n",
    "    model = XGBRegressor()\n",
    "    model.fit(X=train_X, y=train_y)\n",
    "    predicted_y = model.predict(test_X)\n",
    "    \n",
    "    predicted_y_list.append(predicted_y)\n",
    "    \n",
    "    assert len(test_y) == len(predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_corrs(predicted_y, actual_y):\n",
    "    return np.corrcoef(predicted_y, actual_y)[0, 1]\n",
    "\n",
    "def get_mean_returns(predicted_y, actual_y):\n",
    "    return np.sum((np.abs(actual_y) / len(predicted_y)) * \\\n",
    "                  (np.sign(actual_y) * np.sign(predicted_y)))\n",
    "\n",
    "def get_scale_factors(predicted_y, actual_y):\n",
    "    from sklearn import linear_model\n",
    "    \n",
    "    model = linear_model.LinearRegression(fit_intercept=False)\n",
    "    model.fit(X=pd.DataFrame({\"predicted_y\": predicted_y}), y=actual_y)\n",
    "    return model.coef_\n",
    "\n",
    "def get_metric(predicted_y_list, actual_y_list):\n",
    "    \"\"\"Print metrics defind by Scott\n",
    "\n",
    "    Returns: [weighted_corr, mean_return, scale_factor]\n",
    "    \"\"\"\n",
    "    assert len(predicted_y_list) == len(actual_y_list), print(f\"length(predicted_y_list) = {len(predicted_y_list)}, length(actual_y_list) = {len(actual_y_list)}\")\n",
    "    \n",
    "    weighted_corrs, weighted_mean_returns, weighted_scale_factors = [], [], []\n",
    "    for i in range(len(actual_y_list)):\n",
    "        predicted_y, actual_y = predicted_y_list[i], actual_y_list[i]\n",
    "        \n",
    "        weighted_corrs.append(get_response_corrs(predicted_y, actual_y))\n",
    "        weighted_mean_returns.append(get_mean_returns(predicted_y, actual_y))\n",
    "        weighted_scale_factors.append(get_scale_factors(predicted_y, actual_y))\n",
    "    \n",
    "    print(f\"1. Weighted Correlation: {np.mean(weighted_corrs)}\")\n",
    "    print(f\"2. Weighted Mean Return: {np.mean(weighted_mean_returns)}\")\n",
    "    print(f\"3. Weighted Scale Factor: {np.mean(weighted_scale_factors)}\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Weighted Correlation: 0.04051417565014369\n",
      "2. Weighted Mean Return: 0.0002521948540185602\n",
      "3. Weighted Scale Factor: 0.06893561780452728\n"
     ]
    }
   ],
   "source": [
    "get_metric(predicted_y_list, actual_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rrirpnxm_nt_0', 'rrirpnxm_lst15_0', 'rrirpnxm_lsthrx15_0',\n",
       "       'rrirpnxm_toxhr_0', '('relvol_nt_0', 'rrirpnxm_nt_0')',\n",
       "       '('relvol_lst15_0', 'rrirpnxm_lst15_0')',\n",
       "       '('relvol_lsthrx15_0', 'rrirpnxm_lsthrx15_0')',\n",
       "       '('relvol_toxhr_0', 'rrirpnxm_toxhr_0')', 'tonight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
